# Lian-MCP-LLM Agent 系统设计思路

By - Lian - 2025

---

## 初步构想

使用多专家 (多个 llm) 解决问题 (将 llm 封装为类, 每一个 llm session 作为一个实例管理)

使用 **`pydantic` 模型** 管理专家组, 单个专家的参数

## 可实现的核心功能

1. 使用 llm 完成代码创建, 修改任务 (练手使用)

    - 关键点: 使用任务流专家管理流程, 代码专家生成代码, 调用 mcp 执行文件写入

2. 使用 llm 自动整理表单 (导师指点)

    - 关键点: 使用任务流专家管理流程, 阅读专家分析文件内容, 整理专家操作移动文件

3. llm 自动渗透测试 (导师指点)

    - 关键点: 使用任务流专家管理流程, 渗透专家调用工具, 分析结果

4. 接入 `YosaCat` (梦想)

---

## 项目进度

#### | 数据库

使用 `psql`, 基本封装已完成 (等待注释与文档)

本地测试 `python ./test_refactoring.py`

测试连接 `uv run python -m mylib.sql.sql_test`

#### | mcp 包

完成了基本的**mcp 服务器**和**llm 客户端**功能 (纯屎山)

测试 `Server`: `uv run python ./main.py server`

测试 `Client`: `uv run python ./main.py client`

##### 目前 mcp 库集成了 mcp_server 与 llm_client 的功能, 后续需要分离:

-   mcp: 负责 mcp_tools 的实现, 元数据提取, 并且与本地知识库进行联动

-   llm: 负责 llm_chat 的创建, 生命周期管理

#### | Config 包

用于配置加载, 已经完备

> 争议点: 全局单例模式 | 递归加载文件

#### | utils 包

###### 目前实现的包

##### Printer

参考 `mylib.mcp.llm_client` 的实现进行彻底重构

后续支持 `background` `bold` 等显示模式

---

## 🎓 学术使用声明

**本项目为2025届毕业设计作品**

⚠️ **重要提醒**：
- 保护期：2025年11月10日 - 2026年1月19日
- 在此期间，禁止同校同学使用本项目进行毕业设计
- 禁止任何形式的学术作业提交
- 详细条款请参阅 LICENSE 文件

---

## 开发之外

记得抽空去读一读 **任务书** 和 **毕业设计报告模板** ! 还要记得 **读论文** !
